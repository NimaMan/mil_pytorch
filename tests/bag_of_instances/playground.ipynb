{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: CONFIG -\n",
      "n_neurons1:\t15\n",
      "n_neurons2:\t15\n",
      "n_neurons3:\t15\n",
      "learning_rate:\t0.001\n",
      "weight_decay:\t0.001\n",
      "epochs:\t\t4000\n",
      "pos:\t\t50\n",
      "neg:\t\t50\n",
      "class_sep:\t1.0\n",
      "n_features:\t10\n",
      "max_instances:\t15\n",
      "batch_size:\t5\n",
      "patience:\t4000\n",
      "delta:\t\t0\n",
      "INFO: data shape -  torch.Size([100, 15, 10]) 100\n",
      "INFO: Using device: cpu\n",
      "TRAINING:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/kuba/code/aic/mil/mil_pytorch/utils/data_utils.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data[i] = torch.cat([ torch.tensor(instances[ marker : marker + n_instances[i] ]) ,  torch.zeros(max_n_instances - n_instances[i], n_features, dtype = torch.float) ], dim = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/4000] | train_loss: 0.6336758732795715 | valid_loss: 0.8242297172546387 \n",
      "[200/4000] | train_loss: 0.4113631546497345 | valid_loss: 0.813481330871582 \n",
      "[300/4000] | train_loss: 0.2602430582046509 | valid_loss: 0.8234900832176208 \n",
      "[400/4000] | train_loss: 0.18170462548732758 | valid_loss: 0.8219959139823914 \n",
      "[500/4000] | train_loss: 0.14510203897953033 | valid_loss: 0.7961704730987549 \n",
      "[600/4000] | train_loss: 0.12764887511730194 | valid_loss: 0.7721650004386902 \n",
      "[700/4000] | train_loss: 0.11857381463050842 | valid_loss: 0.7600780129432678 \n",
      "[800/4000] | train_loss: 0.11418673396110535 | valid_loss: 0.7551567554473877 \n",
      "[900/4000] | train_loss: 0.11198247224092484 | valid_loss: 0.7547878623008728 \n",
      "[1000/4000] | train_loss: 0.11090278625488281 | valid_loss: 0.7555310726165771 \n",
      "[1100/4000] | train_loss: 0.11044617742300034 | valid_loss: 0.7558886408805847 \n",
      "[1200/4000] | train_loss: 0.11000183969736099 | valid_loss: 0.7497093677520752 \n",
      "[1300/4000] | train_loss: 0.10985008627176285 | valid_loss: 0.7541138529777527 \n",
      "[1400/4000] | train_loss: 0.10985004901885986 | valid_loss: 0.7501609921455383 \n",
      "[1500/4000] | train_loss: 0.10963496565818787 | valid_loss: 0.7549538016319275 \n",
      "[1600/4000] | train_loss: 0.10953124612569809 | valid_loss: 0.7545546293258667 \n",
      "[1700/4000] | train_loss: 0.10948709398508072 | valid_loss: 0.7620428800582886 \n",
      "[1800/4000] | train_loss: 0.10928279161453247 | valid_loss: 0.7590320706367493 \n",
      "[1900/4000] | train_loss: 0.10947730392217636 | valid_loss: 0.7608984708786011 \n",
      "[2000/4000] | train_loss: 0.10925697535276413 | valid_loss: 0.7542930245399475 \n",
      "[2100/4000] | train_loss: 0.10930798202753067 | valid_loss: 0.7498787641525269 \n",
      "[2200/4000] | train_loss: 0.10919895023107529 | valid_loss: 0.7526374459266663 \n",
      "[2300/4000] | train_loss: 0.109135203063488 | valid_loss: 0.7528237104415894 \n",
      "[2400/4000] | train_loss: 0.10911858081817627 | valid_loss: 0.7522228956222534 \n",
      "[2500/4000] | train_loss: 0.10911279171705246 | valid_loss: 0.7541564702987671 \n",
      "[2600/4000] | train_loss: 0.10906258970499039 | valid_loss: 0.7534547448158264 \n",
      "[2700/4000] | train_loss: 0.10909858345985413 | valid_loss: 0.7496799230575562 \n",
      "[2800/4000] | train_loss: 0.1091240644454956 | valid_loss: 0.7503226399421692 \n",
      "[2900/4000] | train_loss: 0.10913575440645218 | valid_loss: 0.7514640688896179 \n",
      "[3000/4000] | train_loss: 0.10914773494005203 | valid_loss: 0.7509686350822449 \n",
      "[3100/4000] | train_loss: 0.1090213879942894 | valid_loss: 0.7451178431510925 \n",
      "[3200/4000] | train_loss: 0.10906585305929184 | valid_loss: 0.7461748719215393 \n",
      "[3300/4000] | train_loss: 0.10916256159543991 | valid_loss: 0.7458231449127197 \n",
      "[3400/4000] | train_loss: 0.10894906520843506 | valid_loss: 0.7436546087265015 \n",
      "[3500/4000] | train_loss: 0.10898635536432266 | valid_loss: 0.743318498134613 \n",
      "[3600/4000] | train_loss: 0.1090623214840889 | valid_loss: 0.7417037487030029 \n",
      "[3700/4000] | train_loss: 0.10911063104867935 | valid_loss: 0.7432650327682495 \n",
      "[3800/4000] | train_loss: 0.10901981592178345 | valid_loss: 0.741121768951416 \n",
      "[3900/4000] | train_loss: 0.10899976640939713 | valid_loss: 0.7402397990226746 \n",
      "[4000/4000] | train_loss: 0.10906735807657242 | valid_loss: 0.7409910559654236 \n",
      "INFO: Finished training - elapsed time: 61.32522916793823\n",
      "EVALUATION:\n",
      "TRAIN\n",
      " - loss_train: 0.10886679308153738\n",
      " - acc_train: 0.95\n",
      " - eer_fpr_train: 0.1\n",
      " - eer_fnr_train: 0.0\n",
      "TEST\n",
      " - loss_test: 0.8989756990549365\n",
      " - acc_test: 0.55\n",
      " - eer_fpr_test: 0.4\n",
      " - eer_fnr_test: 0.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/users/kuba/code/aic/mil')\n",
    "\n",
    "import mil_pytorch.mil as mil\n",
    "from mil_pytorch.utils import eval_utils, data_utils, train_utils, create_bags_simple\n",
    "\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from sklearn.datasets import make_classification\n",
    "import time\n",
    "\n",
    "def create_model(input_len, n_neurons1, n_neurons2, n_neurons3):\n",
    "    # Define neural networks for processing of data before and after aggregation\n",
    "    prepNN1 = torch.nn.Sequential(\n",
    "        torch.nn.Linear(input_len, n_neurons1, bias = True),\n",
    "        torch.nn.ReLU(),\n",
    "        # torch.nn.Linear(n_neurons1, n_neurons2, bias = True),\n",
    "        # torch.nn.ReLU(),\n",
    "    )\n",
    "\n",
    "    afterNN1 = torch.nn.Sequential(\n",
    "        torch.nn.Linear(n_neurons1, 1),\n",
    "        torch.nn.Tanh()\n",
    "    )\n",
    "\n",
    "    # Define model ,loss function and optimizer\n",
    "    model = torch.nn.Sequential(\n",
    "        mil.BagModel_3d(prepNN1, afterNN1, torch.mean, device = device)\n",
    "    ).double()\n",
    "\n",
    "    # Move model to gpu if available\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "# --- CONFIG ---\n",
    "\n",
    "# Configurations\n",
    "n_neurons1 = 15\n",
    "n_neurons2 = 15\n",
    "n_neurons3 = 15\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-3\n",
    "epochs = 4000\n",
    "pos = 50\n",
    "neg = 50\n",
    "class_sep = 1.0\n",
    "n_features = 10\n",
    "max_instances = 15\n",
    "batch_size = 5\n",
    "patience = 4000\n",
    "delta = 0\n",
    "\n",
    "print('INFO: CONFIG -')\n",
    "print('n_neurons1:\\t{}\\nn_neurons2:\\t{}\\nn_neurons3:\\t{}\\nlearning_rate:\\t{}\\nweight_decay:\\t{}\\nepochs:\\t\\t{}\\npos:\\t\\t{}\\nneg:\\t\\t{}\\nclass_sep:\\t{}\\nn_features:\\t{}\\nmax_instances:\\t{}\\nbatch_size:\\t{}\\npatience:\\t{}\\ndelta:\\t\\t{}'.format(n_neurons1, n_neurons2, n_neurons3, learning_rate, weight_decay, epochs, pos, neg, class_sep, n_features, max_instances, batch_size, patience, delta))\n",
    "\n",
    "# --- DATA ---\n",
    "\n",
    "# Create data\n",
    "source_data, source_labels = make_classification(n_samples = 20000, n_features = n_features, n_informative = n_features, n_redundant = 0, n_repeated = 0, n_classes = 10, class_sep = class_sep, n_clusters_per_class = 1)\n",
    "data, ids, labels = create_bags_simple.create_bags(source_data, source_labels, pos = pos, neg = neg, max_instances = max_instances)\n",
    "n_instances = data_utils.ids2n_instances(ids)\n",
    "data_3d = data_utils.create_3d_data(instances = data, n_instances = n_instances)\n",
    "\n",
    "print(\"INFO: data shape - \", data_3d.shape, len(labels))\n",
    "\n",
    "# Check if gpu available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"INFO: Using device: {}\".format(device))\n",
    "\n",
    "# Move data to gpu (if available)\n",
    "data_3d = data_3d.double().to(device)\n",
    "n_instances = n_instances.to(device)\n",
    "labels = labels.long().to(device)\n",
    "\n",
    "# Convert labels from (1, 0) to (1, -1) for tanh\n",
    "labels[labels == 0] = -1\n",
    "\n",
    "# Create dataset and divide to train and test part\n",
    "dataset = mil.MilDataset_3d(data_3d, n_instances, labels, normalize = True)\n",
    "\n",
    "train_indices, valid_indices, test_indices = data_utils.data_split(dataset = dataset, valid_ratio = 0.2, test_ratio = 0.2, shuffle = True, stratify = True)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "# Create dataloaders\n",
    "if batch_size == 0:\n",
    "    train_dl = DataLoader(dataset, sampler = train_sampler, batch_size = len(train_indices))\n",
    "else:\n",
    "    train_dl = DataLoader(dataset, sampler = train_sampler, batch_size = batch_size)\n",
    "\n",
    "valid_dl = DataLoader(dataset, sampler = valid_sampler, batch_size = len(valid_indices))\n",
    "test_dl = DataLoader(dataset, sampler = test_sampler, batch_size = len(test_indices))\n",
    "\n",
    "# --- MODEL ---\n",
    "\n",
    "model = create_model(len(dataset.data[0][0]) , n_neurons1, n_neurons2, n_neurons3)\n",
    "criterion = mil.MyHingeLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "\n",
    "# --- TRAIN ---\n",
    "\n",
    "train_utils.train_model(model, criterion, optimizer, train_dl, valid_dl, epochs, patience, delta)\n",
    "\n",
    "# --- EVAL ---\n",
    "\n",
    "eval_utils.evaluation(model, criterion, train_dl, test_dl, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 15, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_instances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n",
      "torch.Size([5, 15, 10])\n",
      "torch.Size([5])\n",
      "torch.Size([5])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for data, n_instances, labels in train_dl:\n",
    "    print(data.shape)\n",
    "    print(n_instances.shape)\n",
    "    print(labels.shape)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
