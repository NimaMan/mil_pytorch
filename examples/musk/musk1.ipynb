{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/users/kuba/code/aic/mil')\n",
    "\n",
    "import pandas\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import mil_pytorch.mil as mil\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Load data from files\n",
    "data = pandas.read_csv('data.csv', header = None).values\n",
    "ids = pandas.read_csv('ids.csv', squeeze = True, header = None).values\n",
    "labels = pandas.read_csv('labels.csv', squeeze = True, header = None).values\n",
    "\n",
    "# Load data to torch Tensors\n",
    "data = torch.tensor(data)\n",
    "ids = torch.tensor(ids)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create dataset\n",
    "dataset = mil.MilDataset(data, ids, labels, normalize = True)\n",
    "\n",
    "# Create train and test data loaders\n",
    "batch_size = 10\n",
    "\n",
    "indices = numpy.arange(len(dataset))\n",
    "train_indices, test_indices = model_selection.train_test_split(indices, shuffle = True, test_size = 0.2)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_dl = DataLoader(dataset, sampler = train_sampler, batch_size = batch_size, collate_fn=mil.collate)\n",
    "test_dl = DataLoader(dataset, sampler = test_sampler, batch_size = len(test_indices), collate_fn=mil.collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model, criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mil_pytorch.mil as mil\n",
    "\n",
    "# Parameters\n",
    "n_neurons1 = 10\n",
    "n_neurons2 = 10\n",
    "input_len = len(dataset.data[0])\n",
    "\n",
    "# Defining neural networks for proccesing inputs before and after aggregation function\n",
    "prepNN = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_len, n_neurons1, bias = True),\n",
    "    torch.nn.ReLU(),\n",
    ")\n",
    "\n",
    "afterNN = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_neurons2, 1),\n",
    "    torch.nn.Tanh()\n",
    ")\n",
    "\n",
    "# Create model, using custom created prepNN, afterNN and aggregation function\n",
    "model = mil.BagModel(prepNN, afterNN, aggregation_func = torch.mean).double()\n",
    "\n",
    "criterion = mil.MyHingeLoss()\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-6\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING:\n",
      "[100/1000] | train_loss: 0.5030291676521301\n",
      "[200/1000] | train_loss: 0.32362669706344604\n",
      "[300/1000] | train_loss: 0.27514326572418213\n",
      "[400/1000] | train_loss: 0.15280984342098236\n",
      "[500/1000] | train_loss: 0.13485144078731537\n",
      "[600/1000] | train_loss: 0.08008401840925217\n",
      "[700/1000] | train_loss: 0.061587873846292496\n",
      "[800/1000] | train_loss: 0.041280314326286316\n",
      "[900/1000] | train_loss: 0.022021502256393433\n",
      "[1000/1000] | train_loss: 0.016380853950977325\n",
      "Finished training - elapsed time: 19.53723907470703\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "start = time.time()\n",
    "print('TRAINING:')\n",
    "\n",
    "# Empty tensor for collecting losses over batches\n",
    "train_losses = torch.empty(0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Optimization\n",
    "    for data, ids, labels in train_dl:\n",
    "        pred = model((data, ids))\n",
    "        loss = criterion(pred[:,0], labels)\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses = torch.cat((train_losses, loss.float()))\n",
    "\n",
    "    train_loss = torch.mean(train_losses, dim = 0, keepdim = True)\n",
    "    train_losses = torch.empty(0)\n",
    "\n",
    "    # Print message\n",
    "    if (epoch+1)%100 == 0:\n",
    "        print('[{}/{}] | train_loss: {}'.format(epoch+1, epochs, train_loss.item()))\n",
    "\n",
    "print('Finished training - elapsed time: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION:\n",
      "Train data - \n",
      " Loss: 0.011963463061148991\n",
      " Accuracy: 100.00%\n",
      " Equal error rate approximation using false positive rate: 0.0\n",
      " Equal error rate approximation using false negative rate: 0.0\n",
      "Test data - \n",
      " Loss: 0.36846540345879536\n",
      " Accuracy: 78.95%\n",
      " Equal error rate approximation using false positive rate: 0.273\n",
      " Equal error rate approximation using false negative rate: 0.125\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def eer(pred, labels):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(labels.detach(), pred.detach(), pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    EER_fpr = fpr[numpy.nanargmin(numpy.absolute((fnr - fpr)))]\n",
    "    EER_fnr = fnr[numpy.nanargmin(numpy.absolute((fnr - fpr)))]\n",
    "    return EER_fpr, EER_fnr\n",
    "\n",
    "def accuracy(pred, target, threshold = 0):\n",
    "    pred = pred.detach().numpy()\n",
    "    target = target.detach().numpy()\n",
    "\n",
    "    pred[pred >= threshold] = 1\n",
    "    pred[pred < threshold] = -1\n",
    "\n",
    "    return numpy.sum(target == pred)/target.shape[0]\n",
    "\n",
    "print('EVALUATION:')\n",
    "\n",
    "# Train dataloader for evaluation (batch size = size of dataset)\n",
    "train_dl = DataLoader(dataset, sampler = train_sampler, batch_size = len(train_indices), collate_fn=mil.collate)\n",
    "\n",
    "for data, ids, labels in train_dl:\n",
    "    pred = model((data, ids))\n",
    "    loss = criterion(pred[:,0], labels)\n",
    "    acc = accuracy(pred[:,0], labels)\n",
    "    eer_fpr, eer_fnr = eer(pred[:,0], labels)\n",
    "\n",
    "print('Train data - ')\n",
    "print(' Loss: {:6}'.format(loss.item()))\n",
    "print(' Accuracy: {:.2%}'.format(acc))\n",
    "print(' Equal error rate approximation using false positive rate: {:.3}'.format(eer_fpr))\n",
    "print(' Equal error rate approximation using false negative rate: {:.3}'.format(eer_fnr))\n",
    "\n",
    "\n",
    "for data, ids, labels in test_dl:\n",
    "    pred = model((data, ids))\n",
    "    loss = criterion(pred[:,0], labels)\n",
    "    acc = accuracy(pred[:,0], labels)\n",
    "    eer_fpr, eer_fnr = eer(pred[:,0], labels)\n",
    "\n",
    "print('Test data - ')\n",
    "print(' Loss: {:6}'.format(loss.item()))\n",
    "print(' Accuracy: {:.2%}'.format(acc))\n",
    "print(' Equal error rate approximation using false positive rate: {:.3}'.format(eer_fpr))\n",
    "print(' Equal error rate approximation using false negative rate: {:.3}'.format(eer_fnr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
