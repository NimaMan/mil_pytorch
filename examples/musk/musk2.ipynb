{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/users/kuba/code/aic/mil')\n",
    "\n",
    "import pandas\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import mil_pytorch.mil as mil\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Load data from files\n",
    "data = pandas.read_csv('musk2/data.csv', header = None).values\n",
    "ids = pandas.read_csv('musk2/ids.csv', squeeze = True, header = None).values\n",
    "labels = pandas.read_csv('musk2/labels.csv', squeeze = True, header = None).values\n",
    "\n",
    "# Load data to torch Tensors\n",
    "data = torch.tensor(data)\n",
    "ids = torch.tensor(ids)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Create dataset\n",
    "dataset = mil.MilDataset(data, ids, labels, normalize = True)\n",
    "\n",
    "# Create train and test data loaders\n",
    "batch_size = 10\n",
    "\n",
    "indices = numpy.arange(len(dataset))\n",
    "train_indices, test_indices = model_selection.train_test_split(indices, shuffle = True, test_size = 0.2)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_dl = DataLoader(dataset, sampler = train_sampler, batch_size = batch_size, collate_fn=mil.collate)\n",
    "test_dl = DataLoader(dataset, sampler = test_sampler, batch_size = len(test_indices), collate_fn=mil.collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model, criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mil_pytorch.mil as mil\n",
    "\n",
    "# Parameters\n",
    "n_neurons1 = 10\n",
    "n_neurons2 = 10\n",
    "input_len = len(dataset.data[0])\n",
    "\n",
    "# Defining neural networks for proccesing inputs before and after aggregation function\n",
    "prepNN = torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_len, n_neurons1, bias = True),\n",
    "    torch.nn.ReLU(),\n",
    ")\n",
    "\n",
    "afterNN = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_neurons2, 1),\n",
    "    torch.nn.Tanh()\n",
    ")\n",
    "\n",
    "# Create model, using custom created prepNN, afterNN and aggregation function\n",
    "model = mil.BagModel(prepNN, afterNN, aggregation_func = torch.mean).double()\n",
    "\n",
    "criterion = mil.MyHingeLoss()\n",
    "\n",
    "# Optimizer parameters\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-6\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING:\n",
      "[100/3000] | train_loss: 0.6262992024421692\n",
      "[200/3000] | train_loss: 0.40315574407577515\n",
      "[300/3000] | train_loss: 0.2873789966106415\n",
      "[400/3000] | train_loss: 0.2934229075908661\n",
      "[500/3000] | train_loss: 0.21647173166275024\n",
      "[600/3000] | train_loss: 0.17064376175403595\n",
      "[700/3000] | train_loss: 0.14397527277469635\n",
      "[800/3000] | train_loss: 0.12611819803714752\n",
      "[900/3000] | train_loss: 0.10890789330005646\n",
      "[1000/3000] | train_loss: 0.09039434790611267\n",
      "[1100/3000] | train_loss: 0.08774199336767197\n",
      "[1200/3000] | train_loss: 0.06867848336696625\n",
      "[1300/3000] | train_loss: 0.05312468484044075\n",
      "[1400/3000] | train_loss: 0.04415442794561386\n",
      "[1500/3000] | train_loss: 0.03857382759451866\n",
      "[1600/3000] | train_loss: 0.0364614836871624\n",
      "[1700/3000] | train_loss: 0.02362518571317196\n",
      "[1800/3000] | train_loss: 0.04299159348011017\n",
      "[1900/3000] | train_loss: 0.015069548971951008\n",
      "[2000/3000] | train_loss: 0.014552582055330276\n",
      "[2100/3000] | train_loss: 0.01944815181195736\n",
      "[2200/3000] | train_loss: 0.00776492478325963\n",
      "[2300/3000] | train_loss: 0.006471927277743816\n",
      "[2400/3000] | train_loss: 0.005041020456701517\n",
      "[2500/3000] | train_loss: 0.0037553426809608936\n",
      "[2600/3000] | train_loss: 0.003036406822502613\n",
      "[2700/3000] | train_loss: 0.002090341877192259\n",
      "[2800/3000] | train_loss: 0.001614599023014307\n",
      "[2900/3000] | train_loss: 0.0012151503469794989\n",
      "[3000/3000] | train_loss: 0.000965226732660085\n",
      "Finished training - elapsed time: 124.63578915596008\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 3000\n",
    "\n",
    "start = time.time()\n",
    "print('TRAINING:')\n",
    "\n",
    "# Empty tensor for collecting losses over batches\n",
    "train_losses = torch.empty(0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Optimization\n",
    "    for data, ids, labels in train_dl:\n",
    "        pred = model((data, ids))\n",
    "        loss = criterion(pred[:,0], labels)\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses = torch.cat((train_losses, loss.float()))\n",
    "\n",
    "    train_loss = torch.mean(train_losses, dim = 0, keepdim = True)\n",
    "    train_losses = torch.empty(0)\n",
    "\n",
    "    # Print message\n",
    "    if (epoch+1)%100 == 0:\n",
    "        print('[{}/{}] | train_loss: {}'.format(epoch+1, epochs, train_loss.item()))\n",
    "\n",
    "print('Finished training - elapsed time: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION:\n",
      "Train data - \n",
      " Loss: 0.0010248994824667349\n",
      " Accuracy: 100.00%\n",
      " Equal error rate approximation using false positive rate: 0.0\n",
      " Equal error rate approximation using false negative rate: 0.0\n",
      "Test data - \n",
      " Loss: 0.26328833648962274\n",
      " Accuracy: 85.71%\n",
      " Equal error rate approximation using false positive rate: 0.25\n",
      " Equal error rate approximation using false negative rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def eer(pred, labels):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(labels.detach(), pred.detach(), pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    EER_fpr = fpr[numpy.nanargmin(numpy.absolute((fnr - fpr)))]\n",
    "    EER_fnr = fnr[numpy.nanargmin(numpy.absolute((fnr - fpr)))]\n",
    "    return EER_fpr, EER_fnr\n",
    "\n",
    "def accuracy(pred, target, threshold = 0):\n",
    "    pred = pred.detach().numpy()\n",
    "    target = target.detach().numpy()\n",
    "\n",
    "    pred[pred >= threshold] = 1\n",
    "    pred[pred < threshold] = -1\n",
    "\n",
    "    return numpy.sum(target == pred)/target.shape[0]\n",
    "\n",
    "print('EVALUATION:')\n",
    "\n",
    "# Train dataloader for evaluation (batch size = size of dataset)\n",
    "train_dl = DataLoader(dataset, sampler = train_sampler, batch_size = len(train_indices), collate_fn=mil.collate)\n",
    "\n",
    "for data, ids, labels in train_dl:\n",
    "    pred = model((data, ids))\n",
    "    loss = criterion(pred[:,0], labels)\n",
    "    acc = accuracy(pred[:,0], labels)\n",
    "    eer_fpr, eer_fnr = eer(pred[:,0], labels)\n",
    "\n",
    "print('Train data - ')\n",
    "print(' Loss: {:6}'.format(loss.item()))\n",
    "print(' Accuracy: {:.2%}'.format(acc))\n",
    "print(' Equal error rate approximation using false positive rate: {:.3}'.format(eer_fpr))\n",
    "print(' Equal error rate approximation using false negative rate: {:.3}'.format(eer_fnr))\n",
    "\n",
    "\n",
    "for data, ids, labels in test_dl:\n",
    "    pred = model((data, ids))\n",
    "    loss = criterion(pred[:,0], labels)\n",
    "    acc = accuracy(pred[:,0], labels)\n",
    "    eer_fpr, eer_fnr = eer(pred[:,0], labels)\n",
    "\n",
    "print('Test data - ')\n",
    "print(' Loss: {:6}'.format(loss.item()))\n",
    "print(' Accuracy: {:.2%}'.format(acc))\n",
    "print(' Equal error rate approximation using false positive rate: {:.3}'.format(eer_fpr))\n",
    "print(' Equal error rate approximation using false negative rate: {:.3}'.format(eer_fnr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
