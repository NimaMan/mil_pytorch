{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mil_pytorch as mil\n",
    "import utils\n",
    "import train\n",
    "import numpy\n",
    "import math\n",
    "from mill_tree.create_bags import create_bags\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: data normalized\n"
     ]
    }
   ],
   "source": [
    "source_data, source_labels = make_classification(n_samples = 500, n_features = 5, n_informative = 5, n_redundant = 0, n_repeated = 0, n_classes = 10, class_sep = 1.0, n_clusters_per_class = 1)\n",
    "data, ids, labels = create_bags(source_data, source_labels, pos = 10, neg = 10, max_subbags = 3, max_instances = 3)\n",
    "data = torch.Tensor(data).double()\n",
    "ids = torch.Tensor(ids).long()\n",
    "labels = torch.Tensor(labels).long()\n",
    "\n",
    "labels[labels == 0] = -1\n",
    "\n",
    "dataset = mil.MilDataset(data, ids, labels, normalize = True)\n",
    "train_ds, test_ds = mil.train_test_split(dataset, test_size = 0.2)\n",
    "\n",
    "# Dataloader\n",
    "train_dl = DataLoader(train_ds, batch_size = len(train_ds), shuffle = True, collate_fn=mil.collate)\n",
    "train_batch_dl = DataLoader(train_ds, batch_size = 2, shuffle = True, collate_fn=mil.collate)\n",
    "test_dl = DataLoader(test_ds, batch_size = len(test_ds), shuffle = False, collate_fn=mil.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model\n",
    "n_neurons1 = 5\n",
    "n_neurons2 = 5\n",
    "n_neurons3 = 5\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Pre and after agg function\n",
    "prepNN1 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(len(dataset.data[0]), n_neurons1, bias = True),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(n_neurons1, n_neurons2, bias = True),\n",
    "    torch.nn.ReLU()\n",
    ")\n",
    "\n",
    "afterNN1 = torch.nn.Sequential(\n",
    "    torch.nn.Identity()\n",
    ")\n",
    "\n",
    "prepNN2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_neurons2, n_neurons3, bias = True),\n",
    "    torch.nn.ReLU(),\n",
    ")\n",
    "\n",
    "afterNN2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_neurons3, 1),\n",
    "    torch.nn.Tanh()\n",
    ")\n",
    "\n",
    "# Model and loss function\n",
    "model = torch.nn.Sequential(\n",
    "    mil.BagModel(prepNN1, afterNN1, torch.mean),\n",
    "    mil.BagModel(prepNN2, afterNN2, torch.mean)\n",
    ").double()\n",
    "criterion = mil.MyHingeLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  50 Loss: 0.9400529181071849\n",
      "Epoch: 100 Loss: 0.9102542517754277\n",
      "Epoch: 150 Loss: 0.8744165979575649\n",
      "Epoch: 200 Loss: 0.8396825631221034\n",
      "Epoch: 250 Loss: 0.7983179251883162\n",
      "Epoch: 300 Loss: 0.7568962812257061\n",
      "Epoch:  50 Loss: 0.9517688196743607\n",
      "Epoch: 100 Loss: 0.9292619309382159\n",
      "Epoch: 150 Loss: 0.853829873033688\n",
      "Epoch: 200 Loss: 0.6189418009352985\n",
      "Epoch: 250 Loss: 0.3167082446670323\n",
      "Epoch: 300 Loss: 0.2013635143443837\n",
      "Epoch:  50 Loss: 1.0027655846423016\n",
      "Epoch: 100 Loss: 0.8189040557132663\n",
      "Epoch: 150 Loss: 0.7329478685525056\n",
      "Epoch: 200 Loss: 0.69075415672422\n",
      "Epoch: 250 Loss: 0.6356382228941947\n",
      "Epoch: 300 Loss: 0.5383579282532414\n",
      "Epoch:  50 Loss: 0.9899055429398801\n",
      "Epoch: 100 Loss: 0.9733602472024854\n",
      "Epoch: 150 Loss: 0.9480418125885044\n",
      "Epoch: 200 Loss: 0.9145269180843419\n",
      "Epoch: 250 Loss: 0.888476390172592\n",
      "Epoch: 300 Loss: 0.8612287773899024\n",
      "Epoch:  50 Loss: 1.0303363363893427\n",
      "Epoch: 100 Loss: 1.0176172646758257\n",
      "Epoch: 150 Loss: 0.9854142629675872\n",
      "Epoch: 200 Loss: 0.9245918933607234\n",
      "Epoch: 250 Loss: 0.7796576886251437\n",
      "Epoch: 300 Loss: 0.5332614121587007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.1732)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.fit(model, optimizer, criterion, train_dl, test_dl, epochs = 1000)\n",
    "train.k_fold_cv(model, optimizer, criterion, train_ds, epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
